# -*- coding: utf-8 -*-
"""MAX-CUT-EPSDP.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1DXYE4pLZQP9HFm5Yg2DDM_wudjQD_XM4
"""

import time
import numpy as np
import torch
import networkx as nx


# 1 - Creating Max-Cut graph

n = 9000
p = 0.01
seed = 7

G = nx.erdos_renyi_graph(n, p, seed=seed)
E = np.array(list(G.edges()), dtype=int)   # storing the edges of the graph [a,b] ---- etc
m = len(E)
print("nodes:", n, "edges:", m)


# 2 - calculating the cut value from 0/1 labels

def cut_value(x):
    return int(np.sum(x[E[:, 0]] != x[E[:, 1]])) # E[:,0] & E[:1] gives the [a,b] values
                                                 # if diff then sum increases.




# 3 - Goemans–Williamson rounding (vectors -> 0/1 cut)

def gw_rounding(V, rounds=60, seed=0):
    rng = np.random.default_rng(seed)
    Vn = V.detach().cpu().numpy()
    k = Vn.shape[1]
    best = 0
    for _ in range(rounds):
        g = rng.normal(size=k)                # random hyperplane direction
        x01 = (Vn @ g >= 0).astype(int)       # side of hyperplane => group
        best = max(best, cut_value(x01))
    return best

  # already defined and take from the internet


# 4 - EP-SDP core: objective + entropy penalty

device = "cpu"
Et = torch.tensor(E, dtype=torch.long, device=device)  # edges in torch

k = 10                  # rank / embedding dimension
lr = 0.15               # learning rate (alpha)
outer_T = 6             # number of cycles
inner_M = 120           # iterations per stage
lam = 0.1               # lambda start
gamma = 1.5             # lambda multiplier

# Initialize V randomly: shape (n,k)
torch.manual_seed(1)
V = torch.randn(n, k, device=device)

# Row-normalize for easy computation
V = V / (V.norm(dim=1, keepdim=True) + 1e-12)

# Tell PyTorch we want to optimize V (track gradients)
V.requires_grad_(True)

# Optimizer (does gradient steps for us)
optm = torch.optim.Adam([V], lr=lr)

t0 = time.time()

for stage in range(outer_T):
    for it in range(inner_M):
        optm.zero_grad()     # clear old gradients

        # For each edge (i,j), compute dot = v_i * v_j
        vi = V[Et[:, 0]]
        vj = V[Et[:, 1]]
        dots = (vi * vj).sum(dim=1)

        # Score is sum_e (1 - v_i·v_j)/2  (we want to maximize this)
        score = 0.5 * (1.0 - dots).sum()

        # using tsallis entropy
        X = V.T @ V
        trX = torch.trace(X) + 1e-12
        penalty = torch.trace(X @ X) / (trX * trX)

        loss = -(score - lam * penalty) # compute gradiant next step loss
        loss.backward()     # updates the loss
        optm.step()            # updates V

        #normalising the rows in v
        with torch.no_grad():
            V[:] = V / (V.norm(dim=1, keepdim=True) + 1e-12)

    lam *= gamma  # increase lambda after each stage (continuation)

t_ep = time.time() - t0
print("EP-SDP optimize time:", round(t_ep, 3), "sec")

# GW Rounding step
best_cut = gw_rounding(V, rounds=60, seed=2)
print("EP-SDP cut :", best_cut, "out of", m, "edges")

